{"ast":null,"code":"var _jsxFileName = \"/home/sv/LazyHacks/study-tracker-ui/src/pages/SmartAssistant.js\",\n  _s = $RefreshSig$();\n// SmartAssistant.js\n\nimport React, { useState, useRef, useEffect } from 'react';\nimport { useLocation } from 'react-router-dom';\nimport Header from '../components/Header';\nimport './SmartAssistant.css';\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nfunction SmartAssistant() {\n  _s();\n  var _location$state;\n  const location = useLocation();\n  const uploadedFile = (_location$state = location.state) === null || _location$state === void 0 ? void 0 : _location$state.uploadedFile;\n  const [question, setQuestion] = useState(\"\");\n  const [response, setResponse] = useState(\"\");\n  const [isRecording, setIsRecording] = useState(false);\n  const [audioBlob, setAudioBlob] = useState(null);\n  const mediaRecorderRef = useRef(null);\n  const audioURL = audioBlob ? URL.createObjectURL(audioBlob) : null;\n  const [summary, setSummary] = useState(\"\");\n  useEffect(() => {\n    if (uploadedFile === \"MLP.pdf\") {\n      setSummary(`### MLP-Mixer: An All-MLP Architecture for Vision\n\n**Authors**: Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, et al.  \n**Source**: Google Research, Brain Team\n\n---\n\n#### Background\nTraditional computer vision models rely on either **Convolutional Neural Networks (CNNs)**, which apply convolution operations to extract spatial features, or **Vision Transformers (ViTs)**, which use self-attention to capture global dependencies. However, the authors challenge this paradigm by proposing that neither convolutions nor attention mechanisms are strictly necessary to achieve high performance in image classification.\n\n---\n\n#### Introduction to MLP-Mixer\nThe MLP-Mixer is a new approach that relies solely on **Multi-Layer Perceptrons (MLPs)**. The architecture is simple yet innovative, applying MLPs in a way that separates the processing of spatial (token) and channel (feature) information:\n- **Token-Mixing MLPs**: These layers handle spatial information by mixing data across patches in a given feature channel.\n- **Channel-Mixing MLPs**: These layers process information across feature channels at a given spatial location, enabling communication between different channels.\n\n---\n\n#### Mixer Architecture\n1. **Patch Embedding**: The image is divided into fixed-size patches, each projected into a linear embedding. The input representation resembles a table (tokens × channels).\n2. **Mixer Layers**: Each layer consists of:\n   - **Token-Mixing MLP**: Operates across spatial dimensions, taking each column (representing a channel) independently.\n   - **Channel-Mixing MLP**: Operates across feature dimensions, processing each row (representing a token) independently.\n3. **Classifier Head**: Following multiple Mixer layers, global average pooling is applied before the final classification layer.\n\nEach layer also includes **skip-connections** and **layer normalization**, similar to Transformers and ResNets, to improve training stability.\n\n---\n\n#### Key Experiments & Findings\n1. **Performance**: \n   - **ImageNet**: MLP-Mixer achieves competitive results on ImageNet, with accuracy close to Vision Transformers and CNNs.\n   - **Transfer Learning**: When pre-trained on larger datasets like **JFT-300M**, MLP-Mixer achieves even better results, comparable to state-of-the-art ViTs.\n\n2. **Scale and Data**: \n   - The model performs best with large-scale pre-training and benefits significantly from large datasets, which reduce overfitting.\n   - Pre-training on JFT-300M shows that larger Mixer models (e.g., Mixer-H/14) perform as well as, or better than, many Transformer-based models in terms of accuracy and efficiency.\n\n3. **Efficiency**: \n   - The MLP-Mixer is computationally efficient due to its reliance on matrix multiplications and MLPs rather than convolutions or attention.\n   - It provides an **accuracy-compute trade-off** comparable to the best available CNNs and ViTs.\n\n---\n\n#### Advantages Over Traditional Models\n- **Simplicity**: MLP-Mixer avoids convolutions and attention mechanisms, relying only on MLPs.\n- **Efficiency**: Due to its simple operations, the Mixer architecture is less computationally demanding, which may enable faster inference and lower resource consumption.\n\n---\n\n#### Challenges and Limitations\n- **Dependence on Large Data**: Similar to Vision Transformers, the MLP-Mixer requires large datasets to reach its full potential, performing best when pre-trained on tens to hundreds of millions of images.\n- **Overfitting on Smaller Data**: Without large data, MLP-Mixer can overfit, as seen when trained only on ImageNet without additional regularization.\n\n---\n\n#### Conclusion and Future Directions\nThe MLP-Mixer opens new research avenues by questioning the necessity of convolutions and self-attention in computer vision. Its success suggests that **architecture complexity does not always correlate with performance**. Future work might explore the application of Mixer-like architectures in other fields, such as natural language processing or reinforcement learning.\n\nThe authors hope this design inspires further research into simpler models beyond convolutions and self-attention.\n\n`);\n    } else if (uploadedFile === \"rt1.pdf\") {\n      setSummary(\"bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\");\n    } else if (uploadedFile === \"https://arxiv.org/pdf/2406.09246\") {\n      setSummary(\"cccccccccccccccccccccccccccccccc\");\n    } else if (uploadedFile === \"https://arxiv.org/pdf/2312.04560\") {\n      setSummary(\"ddddddddddddddddddddddddd\");\n    } else {\n      setSummary(\"This is a brief summary of the paper's content. (Placeholder text)\");\n    }\n  }, [uploadedFile]);\n  const handleSubmit = () => {\n    const formattedResponse = `Question: ${question || \"audio question transcription\"}\\nAnswer: Thank you for your question! Normally, our system would dynamically search through a sophisticated Retrieval-Augmented Generation (RAG) model, designed to tap into vast knowledge sources, sift through the most relevant information, and deliver an expert answer. But since we only started trying to master AWS yesterday (and found out it’s not exactly a 'learn it overnight' kind of thing), our current setup is a bit... 'manual.' Rest assured, though, once our RAG model is fully operational, your answers will come with speed, precision, and zero cloud confusion!`;\n    setResponse(formattedResponse);\n  };\n  const handleAudioSubmit = async () => {\n    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {\n      console.error(\"getUserMedia is not supported in this browser or context.\");\n      setResponse(\"Error: Unable to access microphone. Please check your browser settings or try another browser.\");\n      return;\n    }\n    try {\n      const permission = await navigator.permissions.query({\n        name: \"microphone\"\n      });\n      if (permission.state === \"denied\") {\n        setResponse(\"Microphone access is denied. Please enable it in your browser settings.\");\n        return;\n      }\n      if (isRecording) {\n        mediaRecorderRef.current.stop();\n        setIsRecording(false);\n      } else {\n        const stream = await navigator.mediaDevices.getUserMedia({\n          audio: true\n        });\n        const mediaRecorder = new MediaRecorder(stream);\n        mediaRecorderRef.current = mediaRecorder;\n        mediaRecorder.start();\n        setIsRecording(true);\n        const audioChunks = [];\n        mediaRecorder.ondataavailable = event => {\n          audioChunks.push(event.data);\n        };\n        mediaRecorder.onstop = () => {\n          const audioBlob = new Blob(audioChunks, {\n            type: 'audio/wav'\n          });\n          setAudioBlob(audioBlob);\n          setIsRecording(false);\n\n          // Simulate audio transcription\n          const simulatedTranscription = \"audio question transcription\";\n          const formattedResponse = `Question: ${simulatedTranscription}\\nAnswer: Thank you for your question! Normally, our system would dynamically search through a sophisticated Retrieval-Augmented Generation (RAG) model, designed to tap into vast knowledge sources, sift through the most relevant information, and deliver an expert answer. But since we only started trying to master AWS yesterday (and found out it’s not exactly a 'learn it overnight' kind of thing), our current setup is a bit... 'manual.' Rest assured, though, once our RAG model is fully operational, your answers will come with speed, precision, and zero cloud confusion!`;\n          setResponse(formattedResponse);\n        };\n      }\n    } catch (error) {\n      console.error(\"Microphone access denied or error: \", error);\n      setResponse(\"Error: Microphone access denied or an error occurred. Please check permissions and try again.\");\n    }\n  };\n  const handleClearAudio = () => {\n    setAudioBlob(null);\n    setQuestion(\"\"); // Reset text input when returning to text input mode\n  };\n  return /*#__PURE__*/_jsxDEV(\"div\", {\n    className: \"smart-assistant-page\",\n    children: [/*#__PURE__*/_jsxDEV(Header, {}, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 154,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"h2\", {\n      children: \"Smart Assistant Summary\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 155,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      className: \"summary\",\n      children: summary\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 156,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"h3\", {\n      children: \"Ask a Question\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 160,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      className: \"question-box\",\n      children: [audioBlob ? /*#__PURE__*/_jsxDEV(\"div\", {\n        className: \"audio-playback\",\n        children: [/*#__PURE__*/_jsxDEV(\"audio\", {\n          controls: true,\n          src: audioURL\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 164,\n          columnNumber: 13\n        }, this), /*#__PURE__*/_jsxDEV(\"button\", {\n          onClick: handleClearAudio,\n          className: \"close-audio-button\",\n          children: \"\\u2716\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 165,\n          columnNumber: 13\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 163,\n        columnNumber: 11\n      }, this) : !isRecording ? /*#__PURE__*/_jsxDEV(\"textarea\", {\n        placeholder: \"Type your question here or use the mic...\",\n        value: question,\n        onChange: e => setQuestion(e.target.value)\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 168,\n        columnNumber: 11\n      }, this) : /*#__PURE__*/_jsxDEV(\"div\", {\n        className: \"recording-indicator\",\n        children: \"Recording... \\uD83D\\uDD34\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 174,\n        columnNumber: 11\n      }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n        className: \"question-box-buttons\",\n        children: [/*#__PURE__*/_jsxDEV(\"button\", {\n          onClick: handleAudioSubmit,\n          className: `mic-button ${isRecording ? 'recording' : ''}`,\n          children: isRecording ? \"Stop Recording\" : \"🎤 Use Mic\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 177,\n          columnNumber: 11\n        }, this), /*#__PURE__*/_jsxDEV(\"button\", {\n          onClick: handleSubmit,\n          className: \"send-button\",\n          disabled: !question.trim() && !audioBlob,\n          children: \"Send\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 180,\n          columnNumber: 11\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 176,\n        columnNumber: 9\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 161,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      className: \"response-box\",\n      children: [/*#__PURE__*/_jsxDEV(\"h4\", {\n        children: \"Assistant Response\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 191,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        className: \"response-text\",\n        children: response\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 192,\n        columnNumber: 9\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 190,\n      columnNumber: 7\n    }, this)]\n  }, void 0, true, {\n    fileName: _jsxFileName,\n    lineNumber: 153,\n    columnNumber: 5\n  }, this);\n}\n_s(SmartAssistant, \"MRgwpWGTRAHAgPDOujkjFfisf0Y=\", false, function () {\n  return [useLocation];\n});\n_c = SmartAssistant;\nexport default SmartAssistant;\nvar _c;\n$RefreshReg$(_c, \"SmartAssistant\");","map":{"version":3,"names":["React","useState","useRef","useEffect","useLocation","Header","jsxDEV","_jsxDEV","SmartAssistant","_s","_location$state","location","uploadedFile","state","question","setQuestion","response","setResponse","isRecording","setIsRecording","audioBlob","setAudioBlob","mediaRecorderRef","audioURL","URL","createObjectURL","summary","setSummary","handleSubmit","formattedResponse","handleAudioSubmit","navigator","mediaDevices","getUserMedia","console","error","permission","permissions","query","name","current","stop","stream","audio","mediaRecorder","MediaRecorder","start","audioChunks","ondataavailable","event","push","data","onstop","Blob","type","simulatedTranscription","handleClearAudio","className","children","fileName","_jsxFileName","lineNumber","columnNumber","controls","src","onClick","placeholder","value","onChange","e","target","disabled","trim","_c","$RefreshReg$"],"sources":["/home/sv/LazyHacks/study-tracker-ui/src/pages/SmartAssistant.js"],"sourcesContent":["// SmartAssistant.js\n\nimport React, { useState, useRef, useEffect } from 'react';\nimport { useLocation } from 'react-router-dom';\nimport Header from '../components/Header';\nimport './SmartAssistant.css';\n\nfunction SmartAssistant() {\n  const location = useLocation();\n  const uploadedFile = location.state?.uploadedFile;\n  const [question, setQuestion] = useState(\"\");\n  const [response, setResponse] = useState(\"\");\n  const [isRecording, setIsRecording] = useState(false);\n  const [audioBlob, setAudioBlob] = useState(null);\n  const mediaRecorderRef = useRef(null);\n  const audioURL = audioBlob ? URL.createObjectURL(audioBlob) : null;\n  const [summary, setSummary] = useState(\"\");\n\n  useEffect(() => {\n    if (uploadedFile === \"MLP.pdf\") {\n        setSummary(`### MLP-Mixer: An All-MLP Architecture for Vision\n\n**Authors**: Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, et al.  \n**Source**: Google Research, Brain Team\n\n---\n\n#### Background\nTraditional computer vision models rely on either **Convolutional Neural Networks (CNNs)**, which apply convolution operations to extract spatial features, or **Vision Transformers (ViTs)**, which use self-attention to capture global dependencies. However, the authors challenge this paradigm by proposing that neither convolutions nor attention mechanisms are strictly necessary to achieve high performance in image classification.\n\n---\n\n#### Introduction to MLP-Mixer\nThe MLP-Mixer is a new approach that relies solely on **Multi-Layer Perceptrons (MLPs)**. The architecture is simple yet innovative, applying MLPs in a way that separates the processing of spatial (token) and channel (feature) information:\n- **Token-Mixing MLPs**: These layers handle spatial information by mixing data across patches in a given feature channel.\n- **Channel-Mixing MLPs**: These layers process information across feature channels at a given spatial location, enabling communication between different channels.\n\n---\n\n#### Mixer Architecture\n1. **Patch Embedding**: The image is divided into fixed-size patches, each projected into a linear embedding. The input representation resembles a table (tokens × channels).\n2. **Mixer Layers**: Each layer consists of:\n   - **Token-Mixing MLP**: Operates across spatial dimensions, taking each column (representing a channel) independently.\n   - **Channel-Mixing MLP**: Operates across feature dimensions, processing each row (representing a token) independently.\n3. **Classifier Head**: Following multiple Mixer layers, global average pooling is applied before the final classification layer.\n\nEach layer also includes **skip-connections** and **layer normalization**, similar to Transformers and ResNets, to improve training stability.\n\n---\n\n#### Key Experiments & Findings\n1. **Performance**: \n   - **ImageNet**: MLP-Mixer achieves competitive results on ImageNet, with accuracy close to Vision Transformers and CNNs.\n   - **Transfer Learning**: When pre-trained on larger datasets like **JFT-300M**, MLP-Mixer achieves even better results, comparable to state-of-the-art ViTs.\n\n2. **Scale and Data**: \n   - The model performs best with large-scale pre-training and benefits significantly from large datasets, which reduce overfitting.\n   - Pre-training on JFT-300M shows that larger Mixer models (e.g., Mixer-H/14) perform as well as, or better than, many Transformer-based models in terms of accuracy and efficiency.\n\n3. **Efficiency**: \n   - The MLP-Mixer is computationally efficient due to its reliance on matrix multiplications and MLPs rather than convolutions or attention.\n   - It provides an **accuracy-compute trade-off** comparable to the best available CNNs and ViTs.\n\n---\n\n#### Advantages Over Traditional Models\n- **Simplicity**: MLP-Mixer avoids convolutions and attention mechanisms, relying only on MLPs.\n- **Efficiency**: Due to its simple operations, the Mixer architecture is less computationally demanding, which may enable faster inference and lower resource consumption.\n\n---\n\n#### Challenges and Limitations\n- **Dependence on Large Data**: Similar to Vision Transformers, the MLP-Mixer requires large datasets to reach its full potential, performing best when pre-trained on tens to hundreds of millions of images.\n- **Overfitting on Smaller Data**: Without large data, MLP-Mixer can overfit, as seen when trained only on ImageNet without additional regularization.\n\n---\n\n#### Conclusion and Future Directions\nThe MLP-Mixer opens new research avenues by questioning the necessity of convolutions and self-attention in computer vision. Its success suggests that **architecture complexity does not always correlate with performance**. Future work might explore the application of Mixer-like architectures in other fields, such as natural language processing or reinforcement learning.\n\nThe authors hope this design inspires further research into simpler models beyond convolutions and self-attention.\n\n`);\n    } else if (uploadedFile === \"rt1.pdf\") {\n        setSummary(\"bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\");\n    } else if (uploadedFile === \"https://arxiv.org/pdf/2406.09246\") {\n        setSummary(\"cccccccccccccccccccccccccccccccc\");\n    } else if (uploadedFile === \"https://arxiv.org/pdf/2312.04560\") {\n        setSummary(\"ddddddddddddddddddddddddd\");\n    } else {\n        setSummary(\"This is a brief summary of the paper's content. (Placeholder text)\");\n    }\n}, [uploadedFile]);\n\n\n  const handleSubmit = () => {\n    const formattedResponse = `Question: ${question || \"audio question transcription\"}\\nAnswer: Thank you for your question! Normally, our system would dynamically search through a sophisticated Retrieval-Augmented Generation (RAG) model, designed to tap into vast knowledge sources, sift through the most relevant information, and deliver an expert answer. But since we only started trying to master AWS yesterday (and found out it’s not exactly a 'learn it overnight' kind of thing), our current setup is a bit... 'manual.' Rest assured, though, once our RAG model is fully operational, your answers will come with speed, precision, and zero cloud confusion!`;\n    setResponse(formattedResponse);\n  };\n\n  const handleAudioSubmit = async () => {\n    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {\n      console.error(\"getUserMedia is not supported in this browser or context.\");\n      setResponse(\"Error: Unable to access microphone. Please check your browser settings or try another browser.\");\n      return;\n    }\n\n    try {\n      const permission = await navigator.permissions.query({ name: \"microphone\" });\n      if (permission.state === \"denied\") {\n        setResponse(\"Microphone access is denied. Please enable it in your browser settings.\");\n        return;\n      }\n\n      if (isRecording) {\n        mediaRecorderRef.current.stop();\n        setIsRecording(false);\n      } else {\n        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n        const mediaRecorder = new MediaRecorder(stream);\n        mediaRecorderRef.current = mediaRecorder;\n        mediaRecorder.start();\n        setIsRecording(true);\n\n        const audioChunks = [];\n        mediaRecorder.ondataavailable = (event) => {\n          audioChunks.push(event.data);\n        };\n\n        mediaRecorder.onstop = () => {\n          const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });\n          setAudioBlob(audioBlob);\n          setIsRecording(false);\n          \n          // Simulate audio transcription\n          const simulatedTranscription = \"audio question transcription\";\n          const formattedResponse = `Question: ${simulatedTranscription}\\nAnswer: Thank you for your question! Normally, our system would dynamically search through a sophisticated Retrieval-Augmented Generation (RAG) model, designed to tap into vast knowledge sources, sift through the most relevant information, and deliver an expert answer. But since we only started trying to master AWS yesterday (and found out it’s not exactly a 'learn it overnight' kind of thing), our current setup is a bit... 'manual.' Rest assured, though, once our RAG model is fully operational, your answers will come with speed, precision, and zero cloud confusion!`;\n          setResponse(formattedResponse);\n        };\n      }\n    } catch (error) {\n      console.error(\"Microphone access denied or error: \", error);\n      setResponse(\"Error: Microphone access denied or an error occurred. Please check permissions and try again.\");\n    }\n  };\n\n  const handleClearAudio = () => {\n    setAudioBlob(null);\n    setQuestion(\"\"); // Reset text input when returning to text input mode\n  };\n\n  return (\n    <div className=\"smart-assistant-page\">\n      <Header />\n      <h2>Smart Assistant Summary</h2>\n      <div className=\"summary\">\n        {summary}\n      </div>\n\n      <h3>Ask a Question</h3>\n      <div className=\"question-box\">\n        {audioBlob ? (\n          <div className=\"audio-playback\">\n            <audio controls src={audioURL} />\n            <button onClick={handleClearAudio} className=\"close-audio-button\">✖</button>\n          </div>\n        ) : !isRecording ? (\n          <textarea\n            placeholder=\"Type your question here or use the mic...\"\n            value={question}\n            onChange={(e) => setQuestion(e.target.value)}\n          />\n        ) : (\n          <div className=\"recording-indicator\">Recording... 🔴</div>\n        )}\n        <div className=\"question-box-buttons\">\n          <button onClick={handleAudioSubmit} className={`mic-button ${isRecording ? 'recording' : ''}`}>\n            {isRecording ? \"Stop Recording\" : \"🎤 Use Mic\"}\n          </button>\n          <button \n            onClick={handleSubmit} \n            className=\"send-button\" \n            disabled={!question.trim() && !audioBlob}\n          >\n            Send\n          </button>\n        </div>\n      </div>\n\n      <div className=\"response-box\">\n        <h4>Assistant Response</h4>\n        <p className=\"response-text\">{response}</p>\n      </div>\n    </div>\n  );\n}\n\nexport default SmartAssistant;\n"],"mappings":";;AAAA;;AAEA,OAAOA,KAAK,IAAIC,QAAQ,EAAEC,MAAM,EAAEC,SAAS,QAAQ,OAAO;AAC1D,SAASC,WAAW,QAAQ,kBAAkB;AAC9C,OAAOC,MAAM,MAAM,sBAAsB;AACzC,OAAO,sBAAsB;AAAC,SAAAC,MAAA,IAAAC,OAAA;AAE9B,SAASC,cAAcA,CAAA,EAAG;EAAAC,EAAA;EAAA,IAAAC,eAAA;EACxB,MAAMC,QAAQ,GAAGP,WAAW,CAAC,CAAC;EAC9B,MAAMQ,YAAY,IAAAF,eAAA,GAAGC,QAAQ,CAACE,KAAK,cAAAH,eAAA,uBAAdA,eAAA,CAAgBE,YAAY;EACjD,MAAM,CAACE,QAAQ,EAAEC,WAAW,CAAC,GAAGd,QAAQ,CAAC,EAAE,CAAC;EAC5C,MAAM,CAACe,QAAQ,EAAEC,WAAW,CAAC,GAAGhB,QAAQ,CAAC,EAAE,CAAC;EAC5C,MAAM,CAACiB,WAAW,EAAEC,cAAc,CAAC,GAAGlB,QAAQ,CAAC,KAAK,CAAC;EACrD,MAAM,CAACmB,SAAS,EAAEC,YAAY,CAAC,GAAGpB,QAAQ,CAAC,IAAI,CAAC;EAChD,MAAMqB,gBAAgB,GAAGpB,MAAM,CAAC,IAAI,CAAC;EACrC,MAAMqB,QAAQ,GAAGH,SAAS,GAAGI,GAAG,CAACC,eAAe,CAACL,SAAS,CAAC,GAAG,IAAI;EAClE,MAAM,CAACM,OAAO,EAAEC,UAAU,CAAC,GAAG1B,QAAQ,CAAC,EAAE,CAAC;EAE1CE,SAAS,CAAC,MAAM;IACd,IAAIS,YAAY,KAAK,SAAS,EAAE;MAC5Be,UAAU,CAAC;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC,CAAC;IACE,CAAC,MAAM,IAAIf,YAAY,KAAK,SAAS,EAAE;MACnCe,UAAU,CAAC,yCAAyC,CAAC;IACzD,CAAC,MAAM,IAAIf,YAAY,KAAK,kCAAkC,EAAE;MAC5De,UAAU,CAAC,kCAAkC,CAAC;IAClD,CAAC,MAAM,IAAIf,YAAY,KAAK,kCAAkC,EAAE;MAC5De,UAAU,CAAC,2BAA2B,CAAC;IAC3C,CAAC,MAAM;MACHA,UAAU,CAAC,oEAAoE,CAAC;IACpF;EACJ,CAAC,EAAE,CAACf,YAAY,CAAC,CAAC;EAGhB,MAAMgB,YAAY,GAAGA,CAAA,KAAM;IACzB,MAAMC,iBAAiB,GAAG,aAAaf,QAAQ,IAAI,8BAA8B,gkBAAgkB;IACjpBG,WAAW,CAACY,iBAAiB,CAAC;EAChC,CAAC;EAED,MAAMC,iBAAiB,GAAG,MAAAA,CAAA,KAAY;IACpC,IAAI,CAACC,SAAS,CAACC,YAAY,IAAI,CAACD,SAAS,CAACC,YAAY,CAACC,YAAY,EAAE;MACnEC,OAAO,CAACC,KAAK,CAAC,2DAA2D,CAAC;MAC1ElB,WAAW,CAAC,gGAAgG,CAAC;MAC7G;IACF;IAEA,IAAI;MACF,MAAMmB,UAAU,GAAG,MAAML,SAAS,CAACM,WAAW,CAACC,KAAK,CAAC;QAAEC,IAAI,EAAE;MAAa,CAAC,CAAC;MAC5E,IAAIH,UAAU,CAACvB,KAAK,KAAK,QAAQ,EAAE;QACjCI,WAAW,CAAC,yEAAyE,CAAC;QACtF;MACF;MAEA,IAAIC,WAAW,EAAE;QACfI,gBAAgB,CAACkB,OAAO,CAACC,IAAI,CAAC,CAAC;QAC/BtB,cAAc,CAAC,KAAK,CAAC;MACvB,CAAC,MAAM;QACL,MAAMuB,MAAM,GAAG,MAAMX,SAAS,CAACC,YAAY,CAACC,YAAY,CAAC;UAAEU,KAAK,EAAE;QAAK,CAAC,CAAC;QACzE,MAAMC,aAAa,GAAG,IAAIC,aAAa,CAACH,MAAM,CAAC;QAC/CpB,gBAAgB,CAACkB,OAAO,GAAGI,aAAa;QACxCA,aAAa,CAACE,KAAK,CAAC,CAAC;QACrB3B,cAAc,CAAC,IAAI,CAAC;QAEpB,MAAM4B,WAAW,GAAG,EAAE;QACtBH,aAAa,CAACI,eAAe,GAAIC,KAAK,IAAK;UACzCF,WAAW,CAACG,IAAI,CAACD,KAAK,CAACE,IAAI,CAAC;QAC9B,CAAC;QAEDP,aAAa,CAACQ,MAAM,GAAG,MAAM;UAC3B,MAAMhC,SAAS,GAAG,IAAIiC,IAAI,CAACN,WAAW,EAAE;YAAEO,IAAI,EAAE;UAAY,CAAC,CAAC;UAC9DjC,YAAY,CAACD,SAAS,CAAC;UACvBD,cAAc,CAAC,KAAK,CAAC;;UAErB;UACA,MAAMoC,sBAAsB,GAAG,8BAA8B;UAC7D,MAAM1B,iBAAiB,GAAG,aAAa0B,sBAAsB,gkBAAgkB;UAC7nBtC,WAAW,CAACY,iBAAiB,CAAC;QAChC,CAAC;MACH;IACF,CAAC,CAAC,OAAOM,KAAK,EAAE;MACdD,OAAO,CAACC,KAAK,CAAC,qCAAqC,EAAEA,KAAK,CAAC;MAC3DlB,WAAW,CAAC,+FAA+F,CAAC;IAC9G;EACF,CAAC;EAED,MAAMuC,gBAAgB,GAAGA,CAAA,KAAM;IAC7BnC,YAAY,CAAC,IAAI,CAAC;IAClBN,WAAW,CAAC,EAAE,CAAC,CAAC,CAAC;EACnB,CAAC;EAED,oBACER,OAAA;IAAKkD,SAAS,EAAC,sBAAsB;IAAAC,QAAA,gBACnCnD,OAAA,CAACF,MAAM;MAAAsD,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAAE,CAAC,eACVvD,OAAA;MAAAmD,QAAA,EAAI;IAAuB;MAAAC,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAAI,CAAC,eAChCvD,OAAA;MAAKkD,SAAS,EAAC,SAAS;MAAAC,QAAA,EACrBhC;IAAO;MAAAiC,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OACL,CAAC,eAENvD,OAAA;MAAAmD,QAAA,EAAI;IAAc;MAAAC,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAAI,CAAC,eACvBvD,OAAA;MAAKkD,SAAS,EAAC,cAAc;MAAAC,QAAA,GAC1BtC,SAAS,gBACRb,OAAA;QAAKkD,SAAS,EAAC,gBAAgB;QAAAC,QAAA,gBAC7BnD,OAAA;UAAOwD,QAAQ;UAACC,GAAG,EAAEzC;QAAS;UAAAoC,QAAA,EAAAC,YAAA;UAAAC,UAAA;UAAAC,YAAA;QAAA,OAAE,CAAC,eACjCvD,OAAA;UAAQ0D,OAAO,EAAET,gBAAiB;UAACC,SAAS,EAAC,oBAAoB;UAAAC,QAAA,EAAC;QAAC;UAAAC,QAAA,EAAAC,YAAA;UAAAC,UAAA;UAAAC,YAAA;QAAA,OAAQ,CAAC;MAAA;QAAAH,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OACzE,CAAC,GACJ,CAAC5C,WAAW,gBACdX,OAAA;QACE2D,WAAW,EAAC,2CAA2C;QACvDC,KAAK,EAAErD,QAAS;QAChBsD,QAAQ,EAAGC,CAAC,IAAKtD,WAAW,CAACsD,CAAC,CAACC,MAAM,CAACH,KAAK;MAAE;QAAAR,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAC9C,CAAC,gBAEFvD,OAAA;QAAKkD,SAAS,EAAC,qBAAqB;QAAAC,QAAA,EAAC;MAAe;QAAAC,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAK,CAC1D,eACDvD,OAAA;QAAKkD,SAAS,EAAC,sBAAsB;QAAAC,QAAA,gBACnCnD,OAAA;UAAQ0D,OAAO,EAAEnC,iBAAkB;UAAC2B,SAAS,EAAE,cAAcvC,WAAW,GAAG,WAAW,GAAG,EAAE,EAAG;UAAAwC,QAAA,EAC3FxC,WAAW,GAAG,gBAAgB,GAAG;QAAY;UAAAyC,QAAA,EAAAC,YAAA;UAAAC,UAAA;UAAAC,YAAA;QAAA,OACxC,CAAC,eACTvD,OAAA;UACE0D,OAAO,EAAErC,YAAa;UACtB6B,SAAS,EAAC,aAAa;UACvBc,QAAQ,EAAE,CAACzD,QAAQ,CAAC0D,IAAI,CAAC,CAAC,IAAI,CAACpD,SAAU;UAAAsC,QAAA,EAC1C;QAED;UAAAC,QAAA,EAAAC,YAAA;UAAAC,UAAA;UAAAC,YAAA;QAAA,OAAQ,CAAC;MAAA;QAAAH,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OACN,CAAC;IAAA;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OACH,CAAC,eAENvD,OAAA;MAAKkD,SAAS,EAAC,cAAc;MAAAC,QAAA,gBAC3BnD,OAAA;QAAAmD,QAAA,EAAI;MAAkB;QAAAC,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAI,CAAC,eAC3BvD,OAAA;QAAGkD,SAAS,EAAC,eAAe;QAAAC,QAAA,EAAE1C;MAAQ;QAAA2C,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAI,CAAC;IAAA;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OACxC,CAAC;EAAA;IAAAH,QAAA,EAAAC,YAAA;IAAAC,UAAA;IAAAC,YAAA;EAAA,OACH,CAAC;AAEV;AAACrD,EAAA,CA5LQD,cAAc;EAAA,QACJJ,WAAW;AAAA;AAAAqE,EAAA,GADrBjE,cAAc;AA8LvB,eAAeA,cAAc;AAAC,IAAAiE,EAAA;AAAAC,YAAA,CAAAD,EAAA","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}